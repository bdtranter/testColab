{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdtranter/testColab/blob/main/Ben_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "gbQvTVP5P6Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement nn.MultiHeadAttention in PyTorch (40 points)"
      ],
      "metadata": {
        "id": "fgFJmXctTVH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an example of using nn.MultiHeadAttention in PyTorch directly\n",
        "\n",
        "# Assuming you have your query, key, and value tensors defined\n",
        "#CHANGED LINE\n",
        "#CHANGED LINE AGAINN!!!\n",
        "# Example:\n",
        "query = torch.randn(10, 32, 128)  # (sequence_length, batch_size, embed_dim)\n",
        "key = torch.randn(10, 32, 128)\n",
        "value = torch.randn(10, 32, 128)\n",
        "\n",
        "mha = nn.MultiheadAttention(embed_dim=128, num_heads=8)\n",
        "\n",
        "# Compute the attention output\n",
        "attn_output, _ = mha(query, key, value, need_weights=False)\n",
        "\n",
        "# attn_output contains the output of the multihead attention\n",
        "print(\"Attention Output Shape:\", attn_output.shape)  # (sequence_length, batch_size, embed_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "G7ybJSB7PKpc",
        "outputId": "fa77cb2e-9c40-4fca-8ced-92dc754a0e1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-417a2b264a04>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#CHANGED LINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (sequence_length, batch_size, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You are required to complete this cell\n",
        "#\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(1)\n",
        "\n",
        "        ##############################TODO (30 points)#######################\n",
        "        # Write code for linear projections.\n",
        "        # You only need three lines of code here, for example:\n",
        "        query = self.query_proj(query)\n",
        "        key = self.key_proj(key)\n",
        "        value = self.value_proj(value)\n",
        "\n",
        "        #####################################################################\n",
        "\n",
        "        # Split into multiple heads\n",
        "        query = query.view(query.size(0), batch_size, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key = key.view(key.size(0), batch_size, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value = value.view(value.size(0), batch_size, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        ##############################TODO (10 points)#######################\n",
        "        # Write code to compute the weighted sum of values\n",
        "\n",
        "        context = torch.matmul(attention_weights, value)\n",
        "        # Hint: Use attention_weights and value to compute context\n",
        "\n",
        "        #####################################################################\n",
        "\n",
        "        # Concatenate heads and project\n",
        "        context = context.transpose(1, 2).contiguous().view(query.size(0), batch_size, self.embed_dim)\n",
        "        output = self.output_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example Usage\n",
        "query = torch.randn(10, 32, 128)  # (sequence_length, batch_size, embed_dim)\n",
        "key = torch.randn(10, 32, 128)\n",
        "value = torch.randn(10, 32, 128)\n",
        "\n",
        "mha = MultiHeadAttention(embed_dim=128, num_heads=8)\n",
        "\n",
        "attn_output = mha(query, key, value)\n",
        "\n",
        "print(\"Attention Output Shape:\", attn_output.shape)  # (sequence_length, batch_size, embed_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVd9HBANT8a_",
        "outputId": "3aa09d40-af00-49fa-b004-d9c4af82b28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output Shape: torch.Size([10, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement nn.TransformerEncoderLayer and nn.TransformerDecoderLayer in PyTorch (30 points)\n",
        "\n",
        "**TransformerEncoderLayer:**\n",
        "\n",
        "1. **Self-Attention:** Applies multi-head self-attention to the input sequence (`src`).\n",
        "2. **Add & Norm:** Adds the output of self-attention to the original input and applies layer normalization.\n",
        "3. **Feedforward:** Passes the normalized output through a feedforward network (two linear layers with an activation function in between).\n",
        "4. **Add & Norm:** Adds the output of the feedforward network to the input of the feedforward layer and applies layer normalization.\n",
        "\n",
        "**TransformerDecoderLayer:**\n",
        "\n",
        "1. **Masked Self-Attention:** Applies masked multi-head self-attention to the decoder input (`tgt`). The mask prevents the decoder from attending to future positions in the sequence.\n",
        "2. **Add & Norm:** Adds the output of masked self-attention to the original decoder input and applies layer normalization.\n",
        "3. **Encoder-Decoder Attention:** Applies multi-head attention with the decoder output as query and the encoder output (`memory`) as key and value.\n",
        "4. **Add & Norm:** Adds the output of encoder-decoder attention to the output of step 2 and applies layer normalization.\n",
        "5. **Feedforward:** Passes the normalized output through a feedforward network (same as in the encoder layer).\n",
        "6. **Add & Norm:** Adds the output of the feedforward network to the input of the feedforward layer and applies layer normalization.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Multi-head Attention:** Allows the model to attend to different parts of the input sequence simultaneously.\n",
        "* **Layer Normalization:** Normalizes the activations within each layer, helping with training stability.\n",
        "* **Feedforward Network:** Provides non-linearity and feature transformation.\n",
        "* **Masking:** Prevents the decoder from attending to future positions during training (masked self-attention)."
      ],
      "metadata": {
        "id": "tz33-5aFVTk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an example of using nn.TransformerEncoderLayer and nn.TransformerDecoderLayer in PyTorch directly\n",
        "\n",
        "# Assuming you have your query, key, and value tensors defined\n",
        "# Example:\n",
        "query = torch.randn(10, 32, 128)  # (sequence_length, batch_size, embed_dim)\n",
        "key = torch.randn(10, 32, 128)\n",
        "value = torch.randn(10, 32, 128)\n",
        "\n",
        "# Define the encoder layer\n",
        "encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
        "\n",
        "# Define the decoder layer\n",
        "decoder_layer = nn.TransformerDecoderLayer(d_model=128, nhead=8)\n",
        "\n",
        "# Pass the query, key, and value through the encoder layer\n",
        "encoder_output = encoder_layer(query)\n",
        "\n",
        "# Pass the encoder output and a target sequence through the decoder layer\n",
        "# Assuming you have a target sequence:\n",
        "target = torch.randn(10, 32, 128)\n",
        "decoder_output = decoder_layer(target, encoder_output)\n",
        "\n",
        "\n",
        "print(\"Encoder Output Shape:\", encoder_output.shape)  # (sequence_length, batch_size, embed_dim)\n",
        "print(\"Decoder Output Shape:\", decoder_output.shape)  # (sequence_length, batch_size, embed_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boW8YpcdPqW1",
        "outputId": "db9c40c2-987d-4dc9-a8a4-25618cdc985b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output Shape: torch.Size([10, 32, 128])\n",
            "Decoder Output Shape: torch.Size([10, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.transformer import _get_activation_fn\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer Encoder layer.\n",
        "\n",
        "    Args:\n",
        "        d_model: The dimension of the input and output embeddings.\n",
        "        nhead: The number of attention heads.\n",
        "        dim_feedforward: The dimension of the feedforward network model.\n",
        "        dropout: The dropout value.\n",
        "        activation: The activation function of intermediate layer, relu or gelu.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        \"\"\"\n",
        "        Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            src: The sequence to the encoder layer (required).\n",
        "            src_mask: The mask for the src sequence (optional).\n",
        "\n",
        "        Shape:\n",
        "            - src: :math:`(S, N, E)`.\n",
        "            - src_mask: :math:`(S, S)`.\n",
        "\n",
        "            - Output: :math:`(S, N, E)`.\n",
        "        \"\"\"\n",
        "        ##################################TODO (10 points)#############################\n",
        "        # Write code to compute src2 as self attention of src\n",
        "        # Your code should be like:\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
        "        # You only need to specify the arguments (replacing ... with concrete arguments) for self.self_attn which is defined in __init__:\n",
        "        # self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        ################################################################################\n",
        "\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer Decoder layer.\n",
        "\n",
        "    Args:\n",
        "        d_model: The dimension of the input and output embeddings.\n",
        "        nhead: The number of attention heads.\n",
        "        dim_feedforward: The dimension of the feedforward network model.\n",
        "        dropout: The dropout value.\n",
        "        activation: The activation function of intermediate layer, relu or gelu.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        \"\"\"\n",
        "        Pass the inputs (and mask) through the decoder layer.\n",
        "\n",
        "        Args:\n",
        "            tgt: The sequence to the decoder layer (required).\n",
        "            memory: The sequence from the last layer of the encoder (required).\n",
        "            tgt_mask: The mask for the tgt sequence (optional).\n",
        "\n",
        "        Shape:\n",
        "            - tgt: :math:`(T, N, E)`.\n",
        "            - memory: :math:`(S, N, E)`.\n",
        "            - tgt_mask: :math:`(T, T)`.\n",
        "            - Output: :math:`(T, N, E)`.\n",
        "        \"\"\"\n",
        "        ##################################TODO (10 points)#############################\n",
        "        # Write code to compute tgt2 as self attention of tgt\n",
        "        # Your code should be like:\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
        "        # tgt2 = self.self_attn(...)[0]\n",
        "        # You only need to specify the arguments (replacing ... with concrete arguments) for self.self_attn which is defined in __init__:\n",
        "        # self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        ################################################################################\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        ##################################TODO (10 points)#############################\n",
        "        # Write code to compute tgt2 as multi head attention of tgt and memory\n",
        "        # Your code should be like:\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
        "        # You only need to specify the arguments (replacing ... with concrete arguments) for self.multihead_attn which is defined in __init__:\n",
        "        # self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        ################################################################################\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "# Assuming you have your input sequence and target sequence\n",
        "# Example:\n",
        "src = torch.randn(10, 32, 128)  # (sequence_length, batch_size, embed_dim)\n",
        "tgt = torch.randn(10, 32, 128)\n",
        "\n",
        "# Instantiate an encoder layer\n",
        "encoder_layer = TransformerEncoderLayer(d_model=128, nhead=8)\n",
        "\n",
        "# Instantiate a decoder layer\n",
        "decoder_layer = TransformerDecoderLayer(d_model=128, nhead=8)\n",
        "\n",
        "# Pass the input through the encoder layer\n",
        "encoder_output = encoder_layer(src)\n",
        "\n",
        "# Pass the target and encoder output through the decoder layer\n",
        "decoder_output = decoder_layer(tgt, encoder_output)\n",
        "\n",
        "# Print the output shapes\n",
        "print(\"Encoder output shape:\", encoder_output.shape)  # Expected: (10, 32, 128)\n",
        "print(\"Decoder output shape:\", decoder_output.shape)  # Expected: (10, 32, 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WgmEpq3V4nT",
        "outputId": "103ba264-9a52-444a-b562-b3dc60e3bb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: torch.Size([10, 32, 128])\n",
            "Decoder output shape: torch.Size([10, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example implementation of nn.TransformerEncoder and nn.TransformerDecoder\n",
        "**TransformerEncoder:**\n",
        "\n",
        "1. **Initialization:**\n",
        "   - Takes an `encoder_layer` (an instance of `TransformerEncoderLayer`) as input, which defines the structure of a single encoder layer.\n",
        "   - `num_layers` specifies the number of encoder layers to stack.\n",
        "   - `norm` is an optional layer normalization layer applied to the final output.\n",
        "2. **Forward Pass:**\n",
        "   - Iterates through the `num_layers` of `encoder_layer` instances.\n",
        "   - Passes the output of the previous layer as input to the current layer.\n",
        "   - Applies the optional `norm` layer to the final output.\n",
        "\n",
        "**TransformerDecoder:**\n",
        "\n",
        "1. **Initialization:**\n",
        "   - Takes a `decoder_layer` (an instance of `TransformerDecoderLayer`) as input, which defines the structure of a single decoder layer.\n",
        "   - `num_layers` specifies the number of decoder layers to stack.\n",
        "   - `norm` is an optional layer normalization layer applied to the final output.\n",
        "2. **Forward Pass:**\n",
        "   - Iterates through the `num_layers` of `decoder_layer` instances.\n",
        "   - Passes the output of the previous layer, along with the encoder output (`memory`), as input to the current layer.\n",
        "   - Applies various masks as needed (e.g., `tgt_mask`, `memory_mask`).\n",
        "   - Applies the optional `norm` layer to the final output.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Stacking Layers:** Both `TransformerEncoder` and `TransformerDecoder` simply stack multiple instances of their respective layer types to create a deeper model.\n",
        "- **Layer Normalization:** The optional `norm` layer helps stabilize training by normalizing the activations within each layer.\n",
        "- **Masking:** The decoder utilizes various masks to control the attention mechanism."
      ],
      "metadata": {
        "id": "wUduuA-TclU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.transformer import _get_clones\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    TransformerEncoder is a stack of N encoder layers.\n",
        "\n",
        "    Args:\n",
        "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
        "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
        "        norm: the layer normalization component (optional).\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        \"\"\"\n",
        "        Pass the input through the encoder layers in turn.\n",
        "\n",
        "        Args:\n",
        "            src: the sequence to the encoder (required).\n",
        "            mask: the mask for the src sequence (optional).\n",
        "\n",
        "        Shape:\n",
        "            - src: :math:`(S, N, E)`.\n",
        "            - mask: :math:`(S, S)`.\n",
        "            - Output: :math:`(S, N, E)`.\n",
        "        \"\"\"\n",
        "        output = src\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, src_mask=mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    TransformerDecoder is a stack of N decoder layers.\n",
        "\n",
        "    Args:\n",
        "        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n",
        "        num_layers: the number of sub-decoder-layers in the decoder (required).\n",
        "        norm: the layer normalization component (optional).\n",
        "    \"\"\"\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        \"\"\"\n",
        "        Pass the inputs (and mask) through the decoder layer in turn.\n",
        "\n",
        "        Args:\n",
        "            tgt: the sequence to the decoder (required).\n",
        "            memory: the sequence from the last layer of the encoder (required).\n",
        "            tgt_mask: the mask for the tgt sequence (optional).\n",
        "            memory_mask: the mask for the memory sequence (optional).\n",
        "\n",
        "        Shape:\n",
        "            - tgt: :math:`(T, N, E)`.\n",
        "            - memory: :math:`(S, N, E)`.\n",
        "            - tgt_mask: :math:`(T, T)`.\n",
        "            - memory_mask: :math:`(T, S)`.\n",
        "            - Output: :math:`(T, N, E)`.\n",
        "        \"\"\"\n",
        "        output = tgt\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                         memory_mask=memory_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ... (assuming you have src, tgt, encoder_layer, decoder_layer from previous example)\n",
        "\n",
        "# Instantiate encoder and decoder\n",
        "encoder = TransformerEncoder(encoder_layer, num_layers=6)\n",
        "decoder = TransformerDecoder(decoder_layer, num_layers=6)\n",
        "\n",
        "# Pass through encoder and decoder\n",
        "encoder_output = encoder(src)\n",
        "decoder_output = decoder(tgt, encoder_output)\n",
        "\n",
        "print(\"Encoder Output Shape (Memory):\", encoder_output.shape)  # (sequence_length, batch_size, embed_dim)\n",
        "print(\"Decoder Output Shape:\", decoder_output.shape)  # (sequence_length, batch_size, embed_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lq9HVUMcuRF",
        "outputId": "41ba49cd-959f-49f6-bd5e-69bf70787fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output Shape (Memory): torch.Size([10, 32, 128])\n",
            "Decoder Output Shape: torch.Size([10, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simplified Transformer model with masking or padding"
      ],
      "metadata": {
        "id": "zDQsyGrGfl_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified Transformer model without masking or padding.\n",
        "\n",
        "    Args:\n",
        "        d_model: the number of expected features in the encoder/decoder inputs (default=512).\n",
        "        nhead: the number of heads in the multiheadattention models (default=8).\n",
        "        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n",
        "        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n",
        "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\"):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "        encoder_norm = nn.LayerNorm(d_model)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Take in and process source/target sequences without masking or padding.\n",
        "\n",
        "        Args:\n",
        "            src: the sequence to the encoder (required).\n",
        "            tgt: the sequence to the decoder (required).\n",
        "\n",
        "        Shape:\n",
        "            - src: :math:`(S, N, E)`.\n",
        "            - tgt: :math:`(T, N, E)`.\n",
        "            - output: :math:`(T, N, E)`.\n",
        "\n",
        "            where S is the source sequence length, T is the target sequence length, N is the\n",
        "            batch size, E is the feature number\n",
        "        \"\"\"\n",
        "\n",
        "        if src.size(1) != tgt.size(1):\n",
        "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "\n",
        "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
        "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
        "\n",
        "        memory = self.encoder(src)\n",
        "        output = self.decoder(tgt, memory)\n",
        "        return output\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initiate parameters in the transformer model.\n",
        "        \"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "\n",
        "# ... (assuming you have src and tgt from previous examples)\n",
        "\n",
        "# Instantiate Transformer\n",
        "transformer_model = Transformer(d_model=128, nhead=8, num_encoder_layers=6, num_decoder_layers=6)\n",
        "\n",
        "# Pass through Transformer (no masks needed)\n",
        "output = transformer_model(src, tgt)\n",
        "\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmHqRNwEQtOx",
        "outputId": "7cfb0896-dee7-4b7c-fef4-b5d7e8ae5b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([10, 32, 128])\n"
          ]
        }
      ]
    }
  ]
}